{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLTK_Exemplos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKcEyiKMvuLS"
      },
      "source": [
        "# Exemplos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N3lQE6LvuLg",
        "outputId": "f728b488-bcc8-4e82-b9d9-eed5ff5e00da"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "text=\"Os que enxergam o copo meio cheio ainda apontarão que várias economias desenvolvidas, como Alemanha, Itália,\" \\\n",
        "\"Canadá, França e Reino Unido, tiveram quedas maiores que a brasileira. No entanto, o infortúnio alheio não reduz \"\\\n",
        "\"em nada nossa própria tragédia. E, em boa parte, a recuperação depende do que for feito internamente, \"\\\n",
        "\"mais que do cenário externo.\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LtWzWN6vuLi"
      },
      "source": [
        "# Tokenização por sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdxadBdqvuLj",
        "outputId": "68bad3b9-c8cf-48f7-e8e4-506d82881f87"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "print(sent_tokenize(text, \"portuguese\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Os que enxergam o copo meio cheio ainda apontarão que várias economias desenvolvidas, como Alemanha, Itália,Canadá, França e Reino Unido, tiveram quedas maiores que a brasileira.', 'No entanto, o infortúnio alheio não reduz em nada nossa própria tragédia.', 'E, em boa parte, a recuperação depende do que for feito internamente, mais que do cenário externo.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZtdDHS4vuLl",
        "outputId": "b2edc521-5b7f-402e-c773-8c8ab68cb541"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(nltk.word_tokenize(text, \"portuguese\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Os', 'que', 'enxergam', 'o', 'copo', 'meio', 'cheio', 'ainda', 'apontarão', 'que', 'várias', 'economias', 'desenvolvidas', ',', 'como', 'Alemanha', ',', 'Itália', ',', 'Canadá', ',', 'França', 'e', 'Reino', 'Unido', ',', 'tiveram', 'quedas', 'maiores', 'que', 'a', 'brasileira', '.', 'No', 'entanto', ',', 'o', 'infortúnio', 'alheio', 'não', 'reduz', 'em', 'nada', 'nossa', 'própria', 'tragédia', '.', 'E', ',', 'em', 'boa', 'parte', ',', 'a', 'recuperação', 'depende', 'do', 'que', 'for', 'feito', 'internamente', ',', 'mais', 'que', 'do', 'cenário', 'externo', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8j5fjegvuLm"
      },
      "source": [
        "# Sinonimos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_edSgXVvuLm",
        "outputId": "97837c7f-44f7-44ca-a052-9f9e510860ae"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "sin = wordnet.synsets(\"intelligent\")\n",
        "print(sin)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('intelligent.a.01'), Synset('intelligent.s.02'), Synset('healthy.s.04'), Synset('intelligent.s.04')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmOxHYS8vuLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a4e59b-06d8-42ca-b2ea-1e4c163fa42a"
      },
      "source": [
        "print(sin[0].definition())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "having the capacity for thought and reason especially to a high degree\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXW7_PuPvuLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f1c430-6369-492e-90fe-26598f88876c"
      },
      "source": [
        "print(sin[0].examples())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is there intelligent life in the universe?', 'an intelligent question']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FThxBHT7vuLp"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAJVARMrvuLp",
        "outputId": "9a0c9f1e-335d-4998-97db-d3bdcf72a234"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"portuguese\")\n",
        "print(stemmer.stem(\"Cantava\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5iw_2JWvuLq",
        "outputId": "13b8e81c-b40b-4269-cad6-d3d646bb11ff"
      },
      "source": [
        "print(stemmer.stem(\"estudante\"))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "estud\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwibAqukvuLr",
        "outputId": "8d9d374f-4a31-41fc-b77c-44401f24834d"
      },
      "source": [
        "print(stemmer.stem(\"dançarina\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dançarin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFmUTDNOvuLr"
      },
      "source": [
        "# Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdsWN_iCvuLr",
        "outputId": "1f0faa5c-6fce-4f5f-9f80-36a4e70b36dd"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "print(lem.lemmatize(\"consulting\", pos = \"v\")) #begin"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "consult\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GhOm1JXvuLs"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn2-EsMrvuLs",
        "outputId": "40b808bb-1aa9-49d0-c7e0-20aa9745d257"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "stopwords.words(\"portuguese\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['de',\n",
              " 'a',\n",
              " 'o',\n",
              " 'que',\n",
              " 'e',\n",
              " 'é',\n",
              " 'do',\n",
              " 'da',\n",
              " 'em',\n",
              " 'um',\n",
              " 'para',\n",
              " 'com',\n",
              " 'não',\n",
              " 'uma',\n",
              " 'os',\n",
              " 'no',\n",
              " 'se',\n",
              " 'na',\n",
              " 'por',\n",
              " 'mais',\n",
              " 'as',\n",
              " 'dos',\n",
              " 'como',\n",
              " 'mas',\n",
              " 'ao',\n",
              " 'ele',\n",
              " 'das',\n",
              " 'à',\n",
              " 'seu',\n",
              " 'sua',\n",
              " 'ou',\n",
              " 'quando',\n",
              " 'muito',\n",
              " 'nos',\n",
              " 'já',\n",
              " 'eu',\n",
              " 'também',\n",
              " 'só',\n",
              " 'pelo',\n",
              " 'pela',\n",
              " 'até',\n",
              " 'isso',\n",
              " 'ela',\n",
              " 'entre',\n",
              " 'depois',\n",
              " 'sem',\n",
              " 'mesmo',\n",
              " 'aos',\n",
              " 'seus',\n",
              " 'quem',\n",
              " 'nas',\n",
              " 'me',\n",
              " 'esse',\n",
              " 'eles',\n",
              " 'você',\n",
              " 'essa',\n",
              " 'num',\n",
              " 'nem',\n",
              " 'suas',\n",
              " 'meu',\n",
              " 'às',\n",
              " 'minha',\n",
              " 'numa',\n",
              " 'pelos',\n",
              " 'elas',\n",
              " 'qual',\n",
              " 'nós',\n",
              " 'lhe',\n",
              " 'deles',\n",
              " 'essas',\n",
              " 'esses',\n",
              " 'pelas',\n",
              " 'este',\n",
              " 'dele',\n",
              " 'tu',\n",
              " 'te',\n",
              " 'vocês',\n",
              " 'vos',\n",
              " 'lhes',\n",
              " 'meus',\n",
              " 'minhas',\n",
              " 'teu',\n",
              " 'tua',\n",
              " 'teus',\n",
              " 'tuas',\n",
              " 'nosso',\n",
              " 'nossa',\n",
              " 'nossos',\n",
              " 'nossas',\n",
              " 'dela',\n",
              " 'delas',\n",
              " 'esta',\n",
              " 'estes',\n",
              " 'estas',\n",
              " 'aquele',\n",
              " 'aquela',\n",
              " 'aqueles',\n",
              " 'aquelas',\n",
              " 'isto',\n",
              " 'aquilo',\n",
              " 'estou',\n",
              " 'está',\n",
              " 'estamos',\n",
              " 'estão',\n",
              " 'estive',\n",
              " 'esteve',\n",
              " 'estivemos',\n",
              " 'estiveram',\n",
              " 'estava',\n",
              " 'estávamos',\n",
              " 'estavam',\n",
              " 'estivera',\n",
              " 'estivéramos',\n",
              " 'esteja',\n",
              " 'estejamos',\n",
              " 'estejam',\n",
              " 'estivesse',\n",
              " 'estivéssemos',\n",
              " 'estivessem',\n",
              " 'estiver',\n",
              " 'estivermos',\n",
              " 'estiverem',\n",
              " 'hei',\n",
              " 'há',\n",
              " 'havemos',\n",
              " 'hão',\n",
              " 'houve',\n",
              " 'houvemos',\n",
              " 'houveram',\n",
              " 'houvera',\n",
              " 'houvéramos',\n",
              " 'haja',\n",
              " 'hajamos',\n",
              " 'hajam',\n",
              " 'houvesse',\n",
              " 'houvéssemos',\n",
              " 'houvessem',\n",
              " 'houver',\n",
              " 'houvermos',\n",
              " 'houverem',\n",
              " 'houverei',\n",
              " 'houverá',\n",
              " 'houveremos',\n",
              " 'houverão',\n",
              " 'houveria',\n",
              " 'houveríamos',\n",
              " 'houveriam',\n",
              " 'sou',\n",
              " 'somos',\n",
              " 'são',\n",
              " 'era',\n",
              " 'éramos',\n",
              " 'eram',\n",
              " 'fui',\n",
              " 'foi',\n",
              " 'fomos',\n",
              " 'foram',\n",
              " 'fora',\n",
              " 'fôramos',\n",
              " 'seja',\n",
              " 'sejamos',\n",
              " 'sejam',\n",
              " 'fosse',\n",
              " 'fôssemos',\n",
              " 'fossem',\n",
              " 'for',\n",
              " 'formos',\n",
              " 'forem',\n",
              " 'serei',\n",
              " 'será',\n",
              " 'seremos',\n",
              " 'serão',\n",
              " 'seria',\n",
              " 'seríamos',\n",
              " 'seriam',\n",
              " 'tenho',\n",
              " 'tem',\n",
              " 'temos',\n",
              " 'tém',\n",
              " 'tinha',\n",
              " 'tínhamos',\n",
              " 'tinham',\n",
              " 'tive',\n",
              " 'teve',\n",
              " 'tivemos',\n",
              " 'tiveram',\n",
              " 'tivera',\n",
              " 'tivéramos',\n",
              " 'tenha',\n",
              " 'tenhamos',\n",
              " 'tenham',\n",
              " 'tivesse',\n",
              " 'tivéssemos',\n",
              " 'tivessem',\n",
              " 'tiver',\n",
              " 'tivermos',\n",
              " 'tiverem',\n",
              " 'terei',\n",
              " 'terá',\n",
              " 'teremos',\n",
              " 'terão',\n",
              " 'teria',\n",
              " 'teríamos',\n",
              " 'teriam']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhRgmKdtvuLt",
        "outputId": "726ea14f-b71d-4050-90df-8c6190cd4158"
      },
      "source": [
        "stw = set(stopwords.words(\"portuguese\"))\n",
        "palavras = word_tokenize(text)\n",
        "filtro = [ ]\n",
        "for p in palavras:\n",
        "    if p not in stw:\n",
        "        filtro.append(p)\n",
        "\n",
        "print(filtro)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Os', 'enxergam', 'copo', 'meio', 'cheio', 'ainda', 'apontarão', 'várias', 'economias', 'desenvolvidas', ',', 'Alemanha', ',', 'Itália', ',', 'Canadá', ',', 'França', 'Reino', 'Unido', ',', 'quedas', 'maiores', 'brasileira', '.', 'No', 'entanto', ',', 'infortúnio', 'alheio', 'reduz', 'nada', 'própria', 'tragédia', '.', 'E', ',', 'boa', 'parte', ',', 'recuperação', 'depende', 'feito', 'internamente', ',', 'cenário', 'externo', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKtbSWKtvuLt"
      },
      "source": [
        "# Retirando pontuação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYnf7TN7vuLu",
        "outputId": "ac6043d5-2532-4b04-e6e2-249a622d7642"
      },
      "source": [
        "from string import punctuation\n",
        "stw = set(stopwords.words(\"portuguese\")+ list(punctuation))\n",
        "palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stw]\n",
        "palavras_sem_stopwords"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Os',\n",
              " 'enxergam',\n",
              " 'copo',\n",
              " 'meio',\n",
              " 'cheio',\n",
              " 'ainda',\n",
              " 'apontarão',\n",
              " 'várias',\n",
              " 'economias',\n",
              " 'desenvolvidas',\n",
              " 'Alemanha',\n",
              " 'Itália',\n",
              " 'Canadá',\n",
              " 'França',\n",
              " 'Reino',\n",
              " 'Unido',\n",
              " 'quedas',\n",
              " 'maiores',\n",
              " 'brasileira',\n",
              " 'No',\n",
              " 'entanto',\n",
              " 'infortúnio',\n",
              " 'alheio',\n",
              " 'reduz',\n",
              " 'nada',\n",
              " 'própria',\n",
              " 'tragédia',\n",
              " 'E',\n",
              " 'boa',\n",
              " 'parte',\n",
              " 'recuperação',\n",
              " 'depende',\n",
              " 'feito',\n",
              " 'internamente',\n",
              " 'cenário',\n",
              " 'externo']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9mMVa-2vuLu"
      },
      "source": [
        "# Classificando palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5_Z3LryvuLv",
        "outputId": "3ab21d12-6e02-4c79-edad-82c0f2e487b1"
      },
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "text = \"the quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "frases =  nltk.sent_tokenize(text)\n",
        "for frase in frases:\n",
        "    print(nltk.pos_tag(nltk.word_tokenize(frase)))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3UFfPfKvuLv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}