# -*- coding: utf-8 -*-
"""NLTK_Exemplos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WITas0ZZTLbiD1oap-M0m1SqB6am6qXp

# Exemplos
"""

import pandas as pd
import nltk

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger') 

text="Os que enxergam o copo meio cheio ainda apontarão que várias economias desenvolvidas, como Alemanha, Itália," \
"Canadá, França e Reino Unido, tiveram quedas maiores que a brasileira. No entanto, o infortúnio alheio não reduz "\
"em nada nossa própria tragédia. E, em boa parte, a recuperação depende do que for feito internamente, "\
"mais que do cenário externo."

"""# Tokenização por sentenças"""

from nltk.tokenize import sent_tokenize
print(sent_tokenize(text, "portuguese"))

from nltk.tokenize import word_tokenize

print(nltk.word_tokenize(text, "portuguese"))

"""# Sinonimos"""

from nltk.corpus import wordnet
sin = wordnet.synsets("intelligent")
print(sin)

print(sin[0].definition())

print(sin[0].examples())

"""# Stemming"""

from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer("portuguese")
print(stemmer.stem("Cantava"))

print(stemmer.stem("estudante"))

print(stemmer.stem("dançarina"))

"""# Lemmatizer"""

from nltk.stem import WordNetLemmatizer
lem = WordNetLemmatizer()
print(lem.lemmatize("consulting", pos = "v")) #begin

"""# Stopwords"""

from nltk.corpus import stopwords
from string import punctuation

stopwords.words("portuguese")

stw = set(stopwords.words("portuguese"))
palavras = word_tokenize(text)
filtro = [ ]
for p in palavras:
    if p not in stw:
        filtro.append(p)

print(filtro)

"""# Retirando pontuação"""

from string import punctuation
stw = set(stopwords.words("portuguese")+ list(punctuation))
palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stw]
palavras_sem_stopwords

"""# Classificando palavras"""

from nltk.tokenize import PunktSentenceTokenizer
text = "the quick brown fox jumps over the lazy dog"

frases =  nltk.sent_tokenize(text)
for frase in frases:
    print(nltk.pos_tag(nltk.word_tokenize(frase)))

